{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Processing Data"
      ],
      "metadata": {
        "id": "ZkoX4MZNguJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "IHx2HrJhRMNK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "zdTvNr9Ggt6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32     # how many independent sequences will we process in parallel?\n",
        "block_size = 8      # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200"
      ],
      "metadata": {
        "id": "ifScSml1sFB7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deGq6uzWRcaF",
        "outputId": "2c1e9cb4-ca03-4722-ac1b-a2617d24d330"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-21 00:37:19--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-05-21 00:37:19 (16.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4OZ5j4_7dxTR"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It8KAEDcelmh",
        "outputId": "027542e3-c143-486b-da1f-2cc0adc3a395"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4Q6E0f8em_s",
        "outputId": "9dac63e3-a78a-46d4-995c-7fe02b6db12b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooiGcr0CeojX",
        "outputId": "b3ded95f-231f-4c57-8f64-55c7ec6d3920"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding the dataset"
      ],
      "metadata": {
        "id": "C847nfoEgpj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "temp_str = \"hii there\"\n",
        "print(\"Encoded list of the string '%s':\" % temp_str, encode(temp_str))\n",
        "print(\"Decoding the encoded list:\", decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnOvpEZ5euNt",
        "outputId": "6e69ce78-83a1-44cc-9dcc-2a1afd62d040"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded list of the string 'hii there': [46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "Decoding the encoded list: hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hw6c7E6ZfflL",
        "outputId": "05a56e94-bf9d-4115-9a92-4e7f4d43395e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting data into Train and Validation sets and Batching"
      ],
      "metadata": {
        "id": "uaqArczMgf8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "z6B29F0-gPyZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8  # context size\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPeEYox2gzJn",
        "outputId": "7b485aac-f6cc-479c-d37a-428c87313344"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFoXBYEuhdTo",
        "outputId": "d9a7011f-0a8b-4500-db24-d158b5e19c7d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split=\"train\"):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_Loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"valid\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "xb, yb = get_batch(split='train')\n",
        "print(f\"Inputs: {xb}\\t{xb.shape}\")\n",
        "print(f\"Targets: {yb}\\t{yb.shape}\")\n",
        "\n",
        "print(70 * \"-\")\n",
        "\n",
        "for b in range(batch_size):     # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b, t]\n",
        "\n",
        "        print(f\"when input is: {context.tolist()}, the target is: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OezODXS2hgMR",
        "outputId": "2800573b-23e0-4cd3-a2dc-4e5449099004"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\ttorch.Size([4, 8])\n",
            "Targets: tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\ttorch.Size([4, 8])\n",
            "----------------------------------------------------------------------\n",
            "when input is: [24], the target is: 43\n",
            "when input is: [24, 43], the target is: 58\n",
            "when input is: [24, 43, 58], the target is: 5\n",
            "when input is: [24, 43, 58, 5], the target is: 57\n",
            "when input is: [24, 43, 58, 5, 57], the target is: 1\n",
            "when input is: [24, 43, 58, 5, 57, 1], the target is: 46\n",
            "when input is: [24, 43, 58, 5, 57, 1, 46], the target is: 43\n",
            "when input is: [24, 43, 58, 5, 57, 1, 46, 43], the target is: 39\n",
            "when input is: [44], the target is: 53\n",
            "when input is: [44, 53], the target is: 56\n",
            "when input is: [44, 53, 56], the target is: 1\n",
            "when input is: [44, 53, 56, 1], the target is: 58\n",
            "when input is: [44, 53, 56, 1, 58], the target is: 46\n",
            "when input is: [44, 53, 56, 1, 58, 46], the target is: 39\n",
            "when input is: [44, 53, 56, 1, 58, 46, 39], the target is: 58\n",
            "when input is: [44, 53, 56, 1, 58, 46, 39, 58], the target is: 1\n",
            "when input is: [52], the target is: 58\n",
            "when input is: [52, 58], the target is: 1\n",
            "when input is: [52, 58, 1], the target is: 58\n",
            "when input is: [52, 58, 1, 58], the target is: 46\n",
            "when input is: [52, 58, 1, 58, 46], the target is: 39\n",
            "when input is: [52, 58, 1, 58, 46, 39], the target is: 58\n",
            "when input is: [52, 58, 1, 58, 46, 39, 58], the target is: 1\n",
            "when input is: [52, 58, 1, 58, 46, 39, 58, 1], the target is: 46\n",
            "when input is: [25], the target is: 17\n",
            "when input is: [25, 17], the target is: 27\n",
            "when input is: [25, 17, 27], the target is: 10\n",
            "when input is: [25, 17, 27, 10], the target is: 0\n",
            "when input is: [25, 17, 27, 10, 0], the target is: 21\n",
            "when input is: [25, 17, 27, 10, 0, 21], the target is: 1\n",
            "when input is: [25, 17, 27, 10, 0, 21, 1], the target is: 54\n",
            "when input is: [25, 17, 27, 10, 0, 21, 1, 54], the target is: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Model"
      ],
      "metadata": {
        "id": "nxbzlVZ1lncD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (B, T) tensors of integers\n",
        "        logits = self.token_embedding_table(idx)    # (B, T, C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "idx = torch.zeros((1, 1), dtype=torch.long)     # 0 tensor serves as a <EOS> token which in our case is a new line character '\\n'\n",
        "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ho7Sn15flad8",
        "outputId": "2b3751a1-2f82-4b8b-aad1-b470b88812b7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "7rpuNW89qCto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "zV5c9qoVlm-M"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_Loss()\n",
        "        print(f\"Step [{iter}]: train loss {losses['train']:.4f}, val loss: {losses['valid']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch(split=\"train\")   # sample a batch\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-L1tZUBqBls",
        "outputId": "c29005f9-d392-4449-da98-9dbfdb874393"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step [0]: train loss 4.7286, val loss: 4.7233\n",
            "Step [500]: train loss 4.1773, val loss: 4.1739\n",
            "Step [1000]: train loss 3.7327, val loss: 3.7328\n",
            "Step [1500]: train loss 3.3876, val loss: 3.3875\n",
            "Step [2000]: train loss 3.1138, val loss: 3.1369\n",
            "Step [2500]: train loss 2.9347, val loss: 2.9356\n",
            "Step [3000]: train loss 2.7902, val loss: 2.8090\n",
            "Step [3500]: train loss 2.7045, val loss: 2.7178\n",
            "Step [4000]: train loss 2.6488, val loss: 2.6430\n",
            "Step [4500]: train loss 2.5934, val loss: 2.6013\n",
            "2.5764429569244385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long)     # 0 tensor serves as a <EOS> token which in our case is a new line character '\\n'\n",
        "print(decode(m.generate(idx, max_new_tokens=400)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf-UYPZRqaJy",
        "outputId": "db67829a-ff65-4a15-e8a6-8e4dd5a64fea"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "y!\n",
            "\n",
            "I'vel,SGUDUCHe hy,\n",
            "\n",
            "corinys&QMADOY'\n",
            "'tr thSStllewl, noisuan os : IN:\n",
            "\n",
            "ThemVOFo?uejQGS:\n",
            "Imy, thack.\n",
            "pAl s VJusuer f t tor r athicke hivmispZ;\n",
            "A\n",
            "a!?jolo.\n",
            "\n",
            "Swhy BYORTI tar\n",
            "\n",
            "FoTowobrt\n",
            "PENED:\n",
            "Fas heandbrn mus:\n",
            "Ty.\n",
            "Vlly y y.\n",
            "I slinis mbCHishadjKIQYO al thangjENCINUEMgq-I:\n",
            "IOFak'eve YDYo-Spstheco, KNGorDO, te, t jusretand s d basorst fine smirIf w.\n",
            "GNTo!\n",
            "\n",
            "IN hindaForolfer s thu moThiswe torthashallSS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 bigram.py"
      ],
      "metadata": {
        "id": "yDGJO4sXq-Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The mathematical trick in self-attention\n",
        "Masked attention"
      ],
      "metadata": {
        "id": "NLwSQziZEmol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B, T, C)    # Batch, time, channels\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSRmsYDhEnAM",
        "outputId": "b787f6ff-503e-4fef-b560-1b31fe5791db"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b, t] = mean_i{i<=t} x[b,i]\n",
        "# version 1\n",
        "\n",
        "xbow = torch.zeros((B, T, C))   # bag of words\n",
        "\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b, :t+1]                  # (t, C)\n",
        "        xbow[b, t] = torch.mean(xprev, 0)   # (1, C)"
      ],
      "metadata": {
        "id": "i4BrDoRuE1Zv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))        # torch.ones(3, 3)\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqARumhJGHDf",
        "outputId": "bcd8e411-3337-44b9-fa23-8bed3a14288f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2\n",
        "\n",
        "wei = torch.tril(torch.ones(T, T))      # weights\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x     # (T, T) @ (B, T, C) -----> Python does Batch mat mul -----> (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2, atol=1e-7)\n",
        "# print(xbow.shape, xbow2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXEjGbZNHfV5",
        "outputId": "414de73a-25f0-4c75-b70a-668e1ce8e740"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3, atol=1e-7)\n",
        "# xbow-xbow3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyNcbLRvJJbN",
        "outputId": "c7f491f6-2833-4c45-8170-5690ebef734d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention\n",
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 32      # batch, time, channels\n",
        "x=torch.randn(B,T,C)\n",
        "\n",
        "# Query: (Roughly speaking) What am I looking for?\n",
        "# Key: (Roughly speaking) What do I contain?\n",
        "# Value: get the affinity between the Query and Key, and this is basically a dot product (similaity check) and it becomes 'wei' in our example\n",
        "\n",
        "# this is a single head of self-attention\n",
        "# bias is equal false, so these only apply a matrix multiplcation.\n",
        "head_size = 16\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "q = query(x)                    # (B, T, 16)\n",
        "k = key(x)                      # (B, T, 16)\n",
        "\n",
        "wei = q @ k.transpose(-2, -1)   # (B, T, 16) @ (B, 16, T) ---> (B, T, T); these give us the affinity of the two layers\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "# wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril==0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = key(x)                      # (B, T, 16)\n",
        "out = wei @ v\n",
        "# out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U79PsEXlhbRv",
        "outputId": "857694ee-31c0-4484-d9db-358270735cc5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 32\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of the self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)                             # (B, T, C)\n",
        "        q = self.query(x)                           # (B, T, C)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5     # (B, T, 16) @ (B, 16, T) ---> (B, T, T); these give us the affinity of the two layers\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))    # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        # performs the weighted aggregation of the values\n",
        "        v = self.value(x)                           # (B, T, C)\n",
        "        out = wei @ v                               # (B, T, T) @ (B, T, C) ---> (B, T, C)\n",
        "        return out\n",
        "\n",
        "# Creating Model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        # self.sa_head = Head(n_embd)\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embd//4)                        # 4 heads of 8-dimentional self-attention\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4)\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # idx and targets are both (B, T) tensors of integers\n",
        "        tok_emb = self.token_embedding_table(idx)                               # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "        x = tok_emb + pos_emb                                                   # (B, T, C)\n",
        "        x = self.sa_heads(x)                                                    # (B, T, C); one head of self-attention\n",
        "        x = self.blocks(x)                                                      # (B, T, C)\n",
        "        logits = self.lm_head(x)                                                # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "DS0LtEh1qznl"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "szyIUzkVtFuV"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linear activation\"\"\"\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_embd, n_embd)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "KyupbzmTJFFn"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer Block: self-attention communication followed by computation \"\"\"\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(x)\n",
        "        x = x + self.ffwd(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "BOg4SgF9LF61"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UQXpV1Ikxq8",
        "outputId": "6547d6c1-f674-43ec-ad3c-d0ecff0cf9ff"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5877, 0.4123, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4457, 0.2810, 0.2733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2220, 0.7496, 0.0175, 0.0109, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0379, 0.0124, 0.0412, 0.0630, 0.8454, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5497, 0.2187, 0.0185, 0.0239, 0.1831, 0.0062, 0.0000, 0.0000],\n",
              "        [0.2576, 0.0830, 0.0946, 0.0241, 0.1273, 0.3627, 0.0507, 0.0000],\n",
              "        [0.0499, 0.1052, 0.0302, 0.0281, 0.1980, 0.2657, 0.1755, 0.1474]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If this is the last row of 'wei': [0.0499, 0.1052, 0.0302, 0.0281, 0.1980, 0.2657, 0.1755, 0.1474]\n",
        "\n",
        "and in particular if we look at 0.1474 (last value; token) it knows what content it has and what position it is in, and now the 8th token is sort of asking the rest of the tokens \"Hey, I am a vowel, I am at 8th position and I am looking for consonants between 0-4\"\n",
        "\n",
        "now all of the nodes will emit keys, and they will say \"I am a consonant, and I am in 0-4 position\" and, after dot product, this will create a high affinity for these two nodes. This means that in softmax, this high affinity will make sure that a lot of information of the 0-4 position node goes through in the embedding, and will learn a lot about it.\n",
        "\n",
        "x is kind of like private information to this token, \"So I am a fifth token and I have some identity and my information is kept in vector x, (and for the purpose of single head) Here is what I am interested in, here is what I have and here is what I will communicate with you if you find me interesting, and this is stored in v (value)\".\n",
        "\n",
        "Notes:\n",
        "\n",
        "- Attention is a communication mechanism. Can be thought of directed graphs.\n",
        "- There is no notion of space in attention, inherently. But we do this on our own to give it some notion of position in the sentence (positional encoding).\n",
        "- Examples (batches), these are independent examples and do not talk to each other."
      ],
      "metadata": {
        "id": "K5kMkh7hlAzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 gpt.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsQM04dHk2xY",
        "outputId": "b0b7caee-e0c6-44e2-bb23-540793e5df4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size: 65, Vocab: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Step [   0]: train loss 4.2000, val loss: 4.2047\n",
            "Step [ 500]: train loss 2.6911, val loss: 2.7087\n",
            "Step [1000]: train loss 2.5196, val loss: 2.5303\n",
            "Step [1500]: train loss 2.4775, val loss: 2.4829\n",
            "Step [2000]: train loss 2.4408, val loss: 2.4523\n",
            "Step [2500]: train loss 2.4272, val loss: 2.4435\n",
            "Step [3000]: train loss 2.4130, val loss: 2.4327\n",
            "Step [3500]: train loss 2.3956, val loss: 2.4212\n",
            "Step [4000]: train loss 2.4041, val loss: 2.3992\n",
            "Step [4500]: train loss 2.3980, val loss: 2.4084\n",
            "\n",
            "Wes le isen.\n",
            "Woto teven INGO, ous into CYedd shou maithe ert thethens the the del ede cksy ow? Wlouby aicecat tisall wor\n",
            "G'imemonou mar ee hacreancad hontrt had wousk ucavere.\n",
            "\n",
            "Baraghe lfousto beme,\n",
            "S m; ten gh;\n",
            "S:\n",
            "Ano ice de bay alysathef beatireplim serbeais I fard\n",
            "Sy,\n",
            "Me hallil:\n",
            "DWAR: us,\n",
            "Wte hse aecathate, parrise in hr'd pat\n",
            "ERY:\n",
            "Bf bul walde betl'ts I yshore grest atre ciak aloo; wo fart het\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After adding multiple head of self-attention"
      ],
      "metadata": {
        "id": "d5bx1hhHscNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 gpt.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xgQid-DsYnW",
        "outputId": "11ce034a-8add-494b-bff8-1b6951718da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size: 65, Vocab: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Step [   0]: train loss 4.2227, val loss: 4.2226\n",
            "Step [ 500]: train loss 2.6592, val loss: 2.6733\n",
            "Step [1000]: train loss 2.4980, val loss: 2.5064\n",
            "Step [1500]: train loss 2.4291, val loss: 2.4349\n",
            "Step [2000]: train loss 2.3716, val loss: 2.3844\n",
            "Step [2500]: train loss 2.3417, val loss: 2.3561\n",
            "Step [3000]: train loss 2.3149, val loss: 2.3347\n",
            "Step [3500]: train loss 2.2918, val loss: 2.3171\n",
            "Step [4000]: train loss 2.2895, val loss: 2.2868\n",
            "Step [4500]: train loss 2.2748, val loss: 2.2858\n",
            "\n",
            "We! le ises.\n",
            "Wmay they row we thutinte Caldd shou mait tiertlentthens the the dol ede cksy ba? Wlouby arceckentisste wre\n",
            "G'imemonot mar ef hacr\n",
            "COngd Go mringt thouskiu?\n",
            "\n",
            "Fre.\n",
            "\n",
            "Bardageplftisto be ess the to hon;\n",
            "Soretr ice we bay, Thouthe wome isspe, laveberis I fald\n",
            "Sy,\n",
            "Whissitill there git; the se aTist,\n",
            "Anos arrise in wito pat\n",
            "ER:\n",
            "Tuche'l walde,\n",
            "Anl'th I yourre grest at west ont of; wonf Gost t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After adding Feed Forward layers"
      ],
      "metadata": {
        "id": "Vcf9kxSVKX_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 gpt.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YakHJmEBKekb",
        "outputId": "ba63495c-720f-46ea-b994-4d6121f7938d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size: 65, Vocab: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Step [   0]: train loss 4.2533, val loss: 4.2552\n",
            "Step [ 500]: train loss 2.6406, val loss: 2.6593\n",
            "Step [1000]: train loss 2.4890, val loss: 2.4925\n",
            "Step [1500]: train loss 2.4175, val loss: 2.4158\n",
            "Step [2000]: train loss 2.3506, val loss: 2.3700\n",
            "Step [2500]: train loss 2.3226, val loss: 2.3445\n",
            "Step [3000]: train loss 2.3080, val loss: 2.3265\n",
            "Step [3500]: train loss 2.2773, val loss: 2.3079\n",
            "Step [4000]: train loss 2.2748, val loss: 2.2823\n",
            "Step [4500]: train loss 2.2550, val loss: 2.2789\n",
            "\n",
            "Boodil thine shall nothe hot mus fin.\n",
            "\n",
            "Gyou rad Ipre, tir frate yand hat lodssell-oolld ave lith so tus noct dand unet, do laded wass:\n",
            "Thouk sto ingrege\n",
            "To pale gaeed and:\n",
            "I wos law sor? fliduace being\n",
            "DE:\n",
            "Yect tountoru--th'd meme cha fles,\n",
            "Tharr, lean poutss:\n",
            "Thim yout ing pearspte, diguls, the souts pof theo calk, is woter that he thawirth tlill mo shat trows,\n",
            "What lound houpeat to mal ryouat, h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After making multiple blocks of Transformers"
      ],
      "metadata": {
        "id": "n7n0245lL_mH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 gpt.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waap1Wb7Kjhi",
        "outputId": "eb1a6d3a-c5e0-48c5-ff4b-5384a2ef4df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size: 65, Vocab: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Step [   0]: train loss 4.1644, val loss: 4.1609\n",
            "Step [ 500]: train loss 3.1764, val loss: 3.1873\n",
            "Step [1000]: train loss 3.0541, val loss: 3.0389\n",
            "Step [1500]: train loss 2.9291, val loss: 2.9113\n",
            "Step [2000]: train loss 2.8216, val loss: 2.8066\n",
            "Step [2500]: train loss 2.7349, val loss: 2.7298\n",
            "Step [3000]: train loss 2.6222, val loss: 2.6188\n",
            "Step [3500]: train loss 2.5914, val loss: 2.5837\n",
            "Step [4000]: train loss 2.5664, val loss: 2.5481\n",
            "Step [4500]: train loss 2.5479, val loss: 2.5253\n",
            "\n",
            "Thed sneedn 'mallrt etn ring; le\n",
            "u,v sortc id'm ned!\n",
            "Iute; mTei hog?\n",
            "\n",
            "BDhord soucnset, an yu sunit anmaoc,\n",
            "Ad ab houd ydad onr ish hoer cu hy hle miur ti, so wre cyeameg'l'l\n",
            "cins\n",
            "Bhy', up hit hocheer sedeult yi, d moud, Treed ivous i au me wa kraul, anann hanws youe:\n",
            "\n",
            "HTe of! I'nw lou Tult thed hatt thi, hed be theps,\n",
            "Thateet?\n",
            "\n",
            "Ydbard etnosem itode huet fid wep he loldt Ylee Hit he gerd ligge;\n",
            "Fre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After adding residual connections and projection layers in the blocks and adding LayerNorm; updated the parameters, added Dropout, and made the network deeper."
      ],
      "metadata": {
        "id": "GnaL1b9COORZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 gpt.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZyADu-YOWaV",
        "outputId": "732d2de6-139c-4120-9eff-396f09322a39"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size: 65, Vocab: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Step [   0]: train loss 4.3779, val loss: 4.3820\n",
            "Step [ 500]: train loss 2.1740, val loss: 2.2213\n",
            "Step [1000]: train loss 1.6603, val loss: 1.8288\n",
            "Step [1500]: train loss 1.4602, val loss: 1.6750\n",
            "Step [2000]: train loss 1.3555, val loss: 1.5945\n",
            "Step [2500]: train loss 1.2883, val loss: 1.5467\n",
            "Step [3000]: train loss 1.2354, val loss: 1.5135\n",
            "Step [3500]: train loss 1.1903, val loss: 1.4974\n",
            "Step [4000]: train loss 1.1544, val loss: 1.4932\n",
            "Step [4500]: train loss 1.1236, val loss: 1.4867\n",
            "\n",
            "SICINIUS:\n",
            "Master you do stay.\n",
            "\n",
            "MAMILLIUS:\n",
            "Such a little tedious justice, nor\n",
            "That straight from made. The reasonable\n",
            "To reus more supple, that you should be case\n",
            "Made beatten to the chase of Lord Tower.\n",
            "\n",
            "BRUTUS:\n",
            "Fatalis, this bidest me Chery thus at hand,\n",
            "That is worse back off yor soaking shalt not death.\n",
            "\n",
            "BENVOLIO:\n",
            "An evil our sister, thre love but rije.\n",
            "Yet, since then being to see thy grace wi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FNPYBNNtXceL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}